{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction of BoardGameGeek Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NAME: Ruochen Chang\n",
    "## ID: 1001780924"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "#### This is a blog illustrates the implementation of Naive Bayes from scratch.Our goal in this blog is to build a classification model to predict the rating of reviews using Naive Bayes.\n",
    "####  I just refered the Naive Bayes model from the Internet and built the classification model from scratch by myself.3\n",
    "#### The basic idea of Naive Bayes is: For a given item to be classified, find the probability of occurrence of each category under the condition that this item appears, whichever is the largest, it is considered that the item to be classified belongs to that category."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes model:\n",
    "$$ P(Y=y_i│X)=\\frac{P(Y=y_i ) ∏_{i=1}d P(Y=y_i)}{P(X)} $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Because all the Y and P(X) are the same, so we can equate the model to such model:\n",
    "$$ P(Y=y_i│X)=arg⁡ maxP(Y=y_i)∏_{i=1} d P(X_i |Y=y_i) $$\n",
    "#### So we need to calculate the probability and conditional probability of our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps to do the Naive Bayes\n",
    "## a. Divide the dataset as train data for 70% and test data for 30%.  \n",
    "### Data Description:\n",
    "#### This review file has 2 columns, comment and rating. \n",
    "#### comment is the review text we should classify\n",
    "#### rating is the score of the reviews.\n",
    "### Our goal is predicting the rating according to the comment text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For this data, the value of data is continuous. So I make them discreet as such rules:\n",
    "#### First, I rounded them to integer number. Then, \n",
    "#### rate as 1 for numbers from 0 to 2;\n",
    "#### rate as 2 for numbers from 3 to 4;\n",
    "#### rate as 3 for numbers from 5 to 6;\n",
    "#### rate as 4 for numbers from 7 to 8;\n",
    "#### rate as 5 for numbers from 9 to 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After loading all the data to the jupyter, I did some pre-processing including text cleaning, tokenization and remove stopwords.\n",
    "#### Our data is often confusing and unintuitive. Therefore, we always have to pre-process the data in a series, which makes the data format more standardized and the content more reasonable. Common data preprocessing methods are: fill in the null value, remove the outliers, data cleaning, tokenization, remove stopwords and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of train data:  1846429\n",
      "length of test data:  791327\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "original_data = pd.read_csv('reviews.csv')\n",
    "all_data = pd.DataFrame(original_data, columns=['comment', 'rating']).dropna()\n",
    "all_data = shuffle(all_data)\n",
    "all_data = pd.DataFrame(all_data).reset_index(drop=True)\n",
    "\n",
    "def round_amount(a):\n",
    "    res = int(float(a))\n",
    "    if res == 0 or res == 1 or res == 2:\n",
    "        label = 0\n",
    "    if res == 3 or res == 4:\n",
    "        label = 1\n",
    "    if res == 5 or res == 6:\n",
    "        label = 2\n",
    "    if res == 7 or res == 8:\n",
    "        label = 3\n",
    "    if res == 9 or res == 10:\n",
    "        label = 4\n",
    "    return label\n",
    "\n",
    "\n",
    "all_data['rating'] = all_data['rating'].apply(round_amount)\n",
    "    \n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "def clean_text(text):\n",
    "    # Make text lowercase, remove text in square brackets,remove links,remove punctuation\n",
    "    # remove words containing numbers.'''\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "# Applying the cleaning function to both test and training datasets\n",
    "all_data['comment'] = all_data['comment'].apply(lambda x: clean_text(x))\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    words = [w for w in text if w not in stopwords.words('english')]\n",
    "    return words\n",
    "\n",
    "train = all_data[:int(0.7*len(all_data))]\n",
    "train = pd.DataFrame(train)\n",
    "test = all_data[int(0.7*len(all_data)):]\n",
    "test = pd.DataFrame(test)\n",
    "print(\"length of train data: \", len(train))\n",
    "print(\"length of test data: \", len(test))\n",
    "# tokenization\n",
    "tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "train['comment'] = train['comment'].apply(lambda x: tokenizer.tokenize(x))\n",
    "test['comment'] = test['comment'].apply(lambda x: tokenizer.tokenize(x))\n",
    "train['comment'] = train['comment'].apply(lambda x: remove_stopwords(x))\n",
    "test['comment'] = test['comment'].apply(lambda x: remove_stopwords(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data:\n",
      "                                             comment  rating\n",
      "0              [game, didnt, suck, didnt, find, fun]       2\n",
      "1  [initial, rating, got, lower, due, long, play,...       3\n",
      "2  [cool, learn, study, games, like, become, cumb...       2\n",
      "3  [probably, oldest, gamers, game, one, showing,...       2\n",
      "4                                  [gets, old, fast]       2\n",
      "\n",
      "\n",
      "test data:\n",
      "                                                   comment  rating\n",
      "1846429  [original, game, gazillon, expansions, game, t...       2\n",
      "1846430  [favourite, version, rummy, theme, works, well...       3\n",
      "1846431  [great, introduction, campaigns, napoleon, sys...       3\n",
      "1846432  [high, trade, value, cards, sleeved, played, l...       3\n",
      "1846433  [hexes, little, restrictive, aside, probably, ...       3\n"
     ]
    }
   ],
   "source": [
    "print(\"train data:\")\n",
    "print(train.head())\n",
    "print(\"\\n\")\n",
    "print(\"test data:\")\n",
    "print(test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b. Build a vocabulary as list. \n",
    "#### Building a vocabulary means build a dictionary for all the words with their occurrence under every label like this: {'happy': [10, 20, 30, 40, 50], ...}. This example means the word happy occurs 10 times under label 1, 20 times under label 2, 30 times under label 3 and so on.\n",
    "#### To be more reasonable, I removed words whose occurrence are less than 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "examples of the vocabulary list:\n",
      "[('game', [43811, 139985, 489289, 808936, 295393]), ('didnt', [1825, 7954, 18987, 14128, 3488]), ('suck', [150, 340, 931, 1118, 293]), ('find', [1175, 4740, 15339, 20548, 7255]), ('fun', [4852, 20851, 103091, 187856, 46340]), ('initial', [207, 1261, 4930, 6804, 1572]), ('rating', [1774, 6197, 23511, 34048, 9241]), ('got', [1498, 4861, 15306, 23082, 7991]), ('lower', [172, 602, 2055, 2843, 659]), ('due', [473, 1849, 6296, 10206, 3242]), ('long', [2694, 10295, 29908, 35439, 11131]), ('play', [11282, 42663, 153352, 232802, 80007]), ('time', [4502, 15269, 50888, 82513, 34209]), ('cool', [586, 2585, 9401, 15694, 3736]), ('learn', [325, 1319, 7205, 20326, 9067]), ('study', [30, 116, 365, 572, 321]), ('games', [7166, 24060, 74610, 116441, 51530]), ('like', [8041, 33504, 111632, 166360, 44658]), ('become', [220, 883, 3694, 6876, 3117]), ('cumbersome', [42, 170, 510, 498, 122])]\n"
     ]
    }
   ],
   "source": [
    "all_words = {}\n",
    "all_s = \"\"\n",
    "for index, row in train.iterrows():\n",
    "    s = \" \".join(row['comment'])\n",
    "    all_s = all_s + s\n",
    "all_words = all_s.lower().split(' ')\n",
    "\n",
    "def count_words(data):\n",
    "    vocabulary_list = {} # {'word':[]}\n",
    "    for index, row in data.iterrows():\n",
    "        for word in row['comment']:\n",
    "            if word not in vocabulary_list:\n",
    "                vocabulary_list[word] = [0, 0, 0, 0, 0]\n",
    "            else:\n",
    "                if row['rating'] == 0:\n",
    "                    vocabulary_list[word][0] += 1\n",
    "                if row['rating'] == 1:\n",
    "                    vocabulary_list[word][1] += 1\n",
    "                if row['rating'] == 2:\n",
    "                    vocabulary_list[word][2] += 1\n",
    "                if row['rating'] == 3:\n",
    "                    vocabulary_list[word][3] += 1\n",
    "                if row['rating'] == 4:\n",
    "                    vocabulary_list[word][4] += 1\n",
    "    for word in list(vocabulary_list.keys()):\n",
    "        if vocabulary_list[word][0]+vocabulary_list[word][1]+vocabulary_list[word][2]+vocabulary_list[word][3]+vocabulary_list[word][4] < 10:\n",
    "            del vocabulary_list[word]\n",
    "    return vocabulary_list\n",
    "\n",
    "vocabulary_list = count_words(train)\n",
    "print('examples of the vocabulary list:')\n",
    "print(list(vocabulary_list.items())[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### write the vocabulary to a txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('data.txt','w')\n",
    "f.write(str(vocabulary_list))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c. Calculate the probability and conditional probability for all the words.\n",
    "\n",
    "#### calculate the total number of every label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[42940, 146435, 535168, 857125, 264761]\n"
     ]
    }
   ],
   "source": [
    "total_length = len(train)\n",
    "    \n",
    "def cal_label_count():\n",
    "    result = []\n",
    "    for i in range(5):\n",
    "        count = 0\n",
    "        for index, row in train.iterrows():\n",
    "            if row['rating'] == i:\n",
    "                count += 1\n",
    "        result.append(count)\n",
    "    return result\n",
    "\n",
    "label_count = cal_label_count()\n",
    "print(label_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Probability of the occurrence: P[word] = num of documents containing this word / num of all documents\n",
    "##### Conditional probability based on the sentiment: P[word | Positive]  = number of positive documents containing this word / num of all positive review documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There are 5 labels totally. So I build a probability list and a conditional probability list to save different 5 labels.\n",
    "### To make our model more reasonable, I used Laplace smoothing to solve the problem of zero probability.\n",
    "## Laplace Smoothing:\n",
    "#### The zero probability problem is that if a certain amount x does not appear in the observation sample library (training set), the result of probability of the entire instance will be 0 when calculating the probability of an instance. In the problem of text classification, when a word does not appear in the training sample, the probability of that word is 0, and it is also 0 when the probability of text occurrence is calculated using multiplication. Clearly, this is unreasonable, and you cannot arbitrarily think that the probability of an event is 0 because it is not observed. In order to solve the problem of zero probability, the French mathematician Laplace first proposed the method of adding 1 to estimate the probability of a phenomenon that a data has not occurred, so this smoothing is also called Laplace smoothing. Assuming that the training sample is very large, the estimated probability change caused by adding 1 to the count of each component x can be ignored, but it can easily and effectively avoid the zero probability problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prior probability:  [0.023256179208138498, 0.07930746509217226, 0.28983922523090455, 0.4642061400515805, 0.14339099041720418]\n",
      "\n",
      "Ocurrence of going word under label 1:  0.030893198579623055\n"
     ]
    }
   ],
   "source": [
    "def cal_prob(i):\n",
    "    count = 0\n",
    "    for index, row in train.iterrows():\n",
    "        if row['rating'] == i:\n",
    "            count += 1\n",
    "    return (count+1)/(len(train)+5)\n",
    "\n",
    "# prior probability\n",
    "prior_list = []\n",
    "for i in range(5):\n",
    "    prior_list.append(cal_prob(i))\n",
    "print(\"prior probability: \", prior_list)  \n",
    "\n",
    "def conditional_prob(word, i):\n",
    "    all_count = label_count[i]\n",
    "    if word in vocabulary_list:\n",
    "        return (vocabulary_list[word][i]+1)/(all_count+5)\n",
    "    if word not in vocabulary_list:\n",
    "        return 1/(all_count+5)\n",
    "\n",
    "print(\"\\nOcurrence of going word under label 1: \", conditional_prob('going', 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d. predict test data\n",
    "#### For test data, we have also pre-processed before, so it is clean data to make prediction. I classified all the test data accroding to our model and print the accuracy. The result of accuracy is about 40%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********predict accuracy*********\n",
      "0.4036574007963838\n"
     ]
    }
   ],
   "source": [
    "def classify(s):\n",
    "    pred_list = []\n",
    "    for i in range(5):\n",
    "        pred = prior_list[i]\n",
    "        for word in s:\n",
    "            newpred = conditional_prob(word, i)\n",
    "            pred *= newpred\n",
    "        pred_list.append(pred)\n",
    "    max_prob = max(pred_list)\n",
    "    return pred_list.index(max_prob)\n",
    "\n",
    "pred_right = 0\n",
    "for index, row in test.iterrows():\n",
    "    if row['rating'] == classify(row['comment']):\n",
    "        pred_right += 1\n",
    "\n",
    "accuracy = pred_right/len(test)\n",
    "print(\"*********predict accuracy*********\")\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge:\n",
    "#### This data is continuous, so I made them discreet. At first, I divided the rating value to 10 grades, but the accuracy is about 20%. So I chose to divide the rating value to 5 grades which is more reasonable because there are so many websites setting the review rating as 5 grades.\n",
    "#### In the future, I want to have a try to build a SVM model and LSTM model to make classification because the time is limited this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
